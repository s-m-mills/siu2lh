<!DOCTYPE html>
<html>
<head>
	<meta name="viewpoint" content="width=device-width, initial-scale=1.0">
	<meta property="og:title" content="Where's the Revolution?"/>
	<meta property="og:url" content="https://siu2lh.com/articles/"/>
	<style>
		body {
			background-color: #000000;
		}
		h1 {color: #000000;}
		p {
			color: #FFFFFF;
			font-family: "Georgia";
		}

		a:link {
			color: #FF7A7A;
			background-color: transparent;
			text-decoration: none;
		}
		
		a:visited {
			color: #FF7A7A;
			background-color: transparent;
			text-decoration: none;
		}

		a:hover {
			color: #00FFFF;
			background-color: transparent;
			text-decoration: none;
		}

		.h1_recentangle {
			height: 80px;
			width: 100%;
			background-color: #000000;
			text-align: center;
			justify-content: center;
			align-items: center;
			display: flex;
			margin: 0;
		}

		.post_window {
			height: 80%;
			width: 50%;
			background-color: #000000;
			border: solid 2px #FFFFFF;
			padding: 50px;
			text-align: justify;
			text-justify: inter-word;
			align-items: center;
			display: flex;
			transition: width 0.5s, height 0.5s;
			box-shadow: 0 0 2px #FFFFFF;
		}

		.post-title {
			font-weight: bold;
			font-size: 24px;
			font-family: "Georgia";
		}

	</style>
	<title>Where's the Revolution?</title>
	<link rel="icon" type="image/x-icon" href="images/favicon.png">
</head>

<body>
	<div class="h1_recentangle" onclick="history.back()"; return false;>
		<h1>
			<a href="#" onclick="history.back()"; return false;><font color="FF7A7A">.</font></a>
		</h1>
	</div>
	<br>
	<center>

		<div class="post_window">
			<p>
				<span class="post-title">Where's the Revolution?</span>
				<br>
				<br>
				<i>Note: Three things. Firstly, in this post I'm mostly focusing on organisational applications of generative AI. I do think there are some radical, transformative perspectives on generative AI in a social sense, and I tried to integrate them here, but it just wasn't working. Inevitably, I'll write something on these ideas up at some point. Secondly, I am somewhat thinking of drafting a book on various ideas contained here and in previous posts which I mentioned here. Those of you who have had to suffer me rant about this stuff for the past couple of years may be not wish to encourage me, but I think I have enough material now to make a good dent in a longer-form piece, and enough avenues of exploration to actually complete that longer-form piece. Thoughts and feedback welcome. Thirdly, a lot of this post is based on an article I hopefully have coming out soon, in a decent journal, and which I will transcripe and post on siu2lh whenever it is out.</i>
				<br>
				<br>
				<b>Introduction</b>
				<br>
				<br>
				A genuinely transformative technology should transform <i>what</i> we do, not just <i>how</i> we do what we're already doing. I don't think any technology can be called revolutionary or radically transformative if this standard is not met.
				<br>
				<br>
				I do not think generative AI is a radically transformative or revolutionary technology.
				<br>
				<br>
				Sufficient time has passed for those AI enthusiasts to demonstrate the transformative potential of these technologies, and the evidence of their failure can be seen in the fact that most of us continue to operate more or less as we did three or four years ago. Some might object to this statement. They, personally, might point to a suite of AI-enabled applications that they are using, or cite various investment data and press releases discussing massive <a href="https://www.ft.com/content/cd6fa1e8-585a-41e1-a521-3efc158b6f94">investment in this or that</a>. But I don't care about any of that. As above, I am interested in whether AI is a genuinely transformative technology. And much of the 'evidence' that has been given to me to support the 'age of AI' just does not meet that standard. The adoption of a new application, AI or otherwise; the investment in new technology, AI or otherwise; the experiments with a new technology, AI or otherwise; these are all parts of the natural, incremental process of organisational evolution which have always occurred. And much of this evolution is a transformation of <i>how we do what we're already doing</i>, rather than transforming <i>what we actually do</i>.
				<br>
				<br>
				<b>Oh Look, Jevons' Paradox</b>
				<br>
				<br>
				Let's examine some studies, and suggested applications, to reveal the incrementalism and wasted application which seems inescapably tied to generative AI. I should start with what is by now a relatively old result. GitClear, a computing consultancy, <a href="https://www.gitclear.com/coding_on_copilot_data_shows_ais_downward_pressure_on_code_quality">examined how much computer code</a> was posted on GitHub, a coding repository, before and after the release of ChatGPT and various coding co-pilot tools around the end of 2022. On the one hand, these tools have likely contributed to a significant increase in the <i>amount</i> of code being written, which by the productivity metric of 'how much code is written' suggests that generative AI is a powerful tool for enhancing programming productivity. On the other hand, the study also found that the amount of code 'churn' increased. Churn is the amount of editing, rewriting, and so on, that must be done to fix broken code. The study thus concluded that generative AI, while writing <i>more</i> code, wrote substantially <i>worse</i> code than the slower, 'less efficient' human programmer would have. Indeed, people were required to then fix the broken code. This, to an extent, <i>could</i> qualify generative AI as a 'radically transformative technology'--it is, after all, changing <i>what</i> some programmers are now doing. But programmers have always fixed broken code. The transformation that has actually happened is that programming with generative AI is now <i>primarily</i> about fixing broken code, rather than writing new code, innovating, playing, and so on. The transformation is from one which requires skills to <i>generate</i> coding solutions to problems, to one that requires programming knowlegde simply to fix stuff. <a href="https://en.wikipedia.org/wiki/Bullshit_Jobs">Duct-taping</a> is one word for it--in fact, <i>fixing broken code in the Wikipedia example of it</i>.
				<br>
				<br>
				Office applications of generaitve AI is another fun area where one could see the results as supporting 'massive' productivity increases, or as masking the failure of the technology to be radically transformative. Microsoft recently released their <a href="https://www.microsoft.com/en-us/research/uploads/prodnew/2024/07/Generative-AI-in-Real-World-Workplaces.pdf">second productivity report</a>. As one might imagine from a company with such substantial investments in AI--not to mention interests in terms of <a href="https://www.ft.com/content/7ca3a8a2-7660-4da3-a19e-1003e6cf45db">data centres and cloud computing</a>--this report suggested that "generative AI is already aiding workers in becoming more productive in their day-to-day jobs in significant ways." This is an interesting conclusion as even the authors seem a little less confident of this conclusion when one digs into the findings. They note, for instance, that there appears to be a lot of sector variation which requires further research.
				<br>
				<br>
				One result, investigating the number of meetings AI co-pilot users attended, versus non-AI participants, finds no firm conclusions <i>except</i> that AI usage seems to have created attendance volatility: some significantly reduced their meeting attendance, while others saw substantial increases. Naturally, the authors suggest this change arises because AI transcription tools are so good people can afford to skip meetings, <i>and</i> because copilot integration may make meetings more efficient (allowing more of them to be scheduled in a given time), and more effective (incentivising more of them). Let's deal with this curious result before moving onto the other metrics. If AI transcription means a person doesn't need to attend the meeting, this implies that the meeting was not necessary to begin with. The purpose of a meeting should be to contribute, share, and question ideas. It is in this human interaction that meetings become valuable, dare I say <i>productive</i>. If the value of the meeting is simply in sharing information, the meeting was not necessary to begin with (this is an example of what I have <a href="https://theconversation.com/chatgpt-why-it-will-probably-remain-just-a-tool-that-does-inefficient-work-more-efficiently-201315">previously called</a> 'efficient ineffiency'). A well-crafted email will probably be better than an AI generated transcription of a poorly-attended meeting. So, where is the productivity gain here? Where is the radical transformation? If, on the other hand, AI is making meetings more efficient, allowing <i>more</i> meetings to be scheduled; this is not necessarily productivity enhancing either! It's a classic case of <a href="https://en.wikipedia.org/wiki/Jevons_paradox">Jevons' paradox</a>--the economic phenomenon when efficiency gains cause the less-required resource to actually be demanded <i>more</i>. There is no reason to think that these new meetings are offering any benefits to productivity. It's a silly truism, but it must be stated: AI is not productivity enhancing if the time that AI saves you is used to do unproductive things. Indeed, let's combine these two stories. Where is the 'efficiency' coming from? One hypothesis we could consider is that AI transcription means people need to ask further questions because they automatically have notes created for them, rather than potentially missing things when they are first discussed. Great! But again--doesn't this suggest that a well-crafted email would have <i>always</i> been better than scheduling the meeting and bringing along your AI personal assistant to take notes?
				<br>
				<br>
				The Microsoft report has two other results which are interesting. Firstly, they find that those using the AI copilot read significantly fewer emails than those non-AI participants. I do not know if the copilot in the study includes a feature like <a href="https://www.wheresyoured.at/untitled/">Apple's floated 'auto-reply' email feature</a>, so let's first assume it does not. In this case, does this not imply that most emails being sent are not worth reading? Again, the value of an email is not in its being sent, but in the human interaction as people communicate worthwhile information. If an email can be ignored, and there being no consequence for doing so, the email was not worth sending to begin with. Now, let's assume there is some auto-reply feature (this is the only reason why I could imagine AI would lead to a reduction in the number of emails being read). This being so, the problem just becomes worse! Not only does this suggest that an email is not worth reading (because this was delegated to an AI); it suggests that the email response is not worth reading <i>because</i> an AI, rather than a person, 'read' it. I reiterate: the value of an email, of <i>any</i> communication, is in the interpersonal interaction, not in the <i>performative</i> act of communication.
				<br>
				<br>
				Finally, the report discusses the number of Word, Excel, and PowerPoint documents being created and edited. While non-AI users didn't really change their habits in this regard; AI copilot users created and edited <i>significantly</i> more documents. Again, putting a positive spin on things, the authors suggest this is because high-quality text generation meant there was just more valuable stuff for participants to create, leading to the production of documents they would have never dreamed of producing. I'm quite sceptical of this. Readers of siu2lh will know of <a href="https://siu2lh.com/articles/empowering-everyday-experts.html">my Herbert Simon teleprinter example</a>, so I will not repeat it here. Instead, I'll talk about Chilean public administration in the 1960s. Between 1962, when Chile had essentially no computers, and 1969, when they had some, the number of government report pages produced increased six-fold. Some of this increase will, undoubtedly, be from new insights gleamed from computering technology (e.g., new statistical analyses of data that were previously too onerous to produce). But much of this, as above, is Jevons' paradox again--the 'cost' of producing documents fell so much that more documents were demanded than had ever previously been demanded. I suspect this Microsoft report has found a similar effect. So, the question is: are these AI 'supported' documents <i>actually</i> enhancing productivity? Who knows, but I would guess they are not. Again, AI is efficiency enhancing if that document which took you an hour to write now takes only ten minutes, and you can spend the other 50 doing <i>something else</i> productive. It is not efficiency enhancing if now you produce six versions of that same document in the same period of time. (Note: I would be interested in any metrics like the Chile example around document production.)
				<br>
				<br>
				<b>Generative AI's Duct-Taping Problem</b>
				<br>
				<br>
				Here in lies the connecting branch between productivity, efficiency, and radical transformation. If AI is creating substantial productivity gains, it should allow people the time and the resource to <i>do other things</i>. To do <i>radically</i> different things. Language, writing, the steam engine, electricity, the computer (to a lesser extent) are radically transformative technologies because they fundamentally shift <i>what</i> it is we as a society are engaged in because <i>they create new opportunities</i>. As above, AI is transforming roles, but not in a radical way. Instead, rather than writing the document, you are editing the poorly written AI document, fixing the poorly written AI code, reading the transcription of a conversation that previously you yourself would have been involved in (and perhaps, maybe, could have contributed to). As I <a href="https://siu2lh.com/articles/generative-ai-does-not-hallucinate.html">recently wrote</a> in relation to AI 'hallucinations', these transformative effects are really just exploitations of human adaptability given the rigidity of AI processes and the limits of information technology.
				<br>
				<br>
				Earlier I used the term <a href="https://en.wikipedia.org/wiki/Bullshit_Jobs">'duct-taper.'</a> This is a term coined by David Graeber to describe someone whose job only exists to temporarily solve a problem which could be permanently solved, but for whatever reason someone is choosing not to. The coding example suggests that generative AI, rather than <i>producing</i> productive benefits, is often just transforming jobs into duct-taping roles. In other instances, though, I think generative AI is being utilised to <i>perform</i> duct-taping. Here are two examples. There is some hype around <a href="https://www.forbes.com/sites/bernardmarr/2024/01/26/how-generative-ai-is-revolutionizing-customer-service/">generative AI customer service agents</a>. These chatbots are useful insofar as most customer service queries are standard, repetitive, and have immediately available solutions. It is perhaps not worth a person's time to have them engaged in this work (of course, then the firm fires this person, because that is the only way AI realises 'efficiency' gains for the company). The minority are obscure, complex problems which perhaps require a bespoke solution. In these instances, it is worth employing an adaptable person to find viable solutions. Great. But... if you have so many routine customer service problems, why not just improve your service so these problems don't occur in the first place? Another example I have recently seen is using generative AI chatbots to alleviate feelings of loneliness and provide related mental health services. <a href="https://www.hbs.edu/faculty/Pages/item.aspx?num=66065">Some studies suggest</a> that these applications can have positive effects on people's lives, and I'm not going to knock something if it works for you. But... if you have so many people feeling lonely that they need mental healthcare, why not just invest in community projects and healthcare? (Note: I discovered that Sherry Turkle, an academic I greatly admire, has previously <a href="https://news.harvard.edu/gazette/story/2024/03/lifting-a-few-with-my-chatbot/">issued warnings</a> about relying on AI companionship. Amongst writing the brilliant <a href="https://direct.mit.edu/books/monograph/2327/The-Second-SelfComputers-and-the-Human-Spirit"><i>The Second Self</i></a> at the drawn on the popular computing movement in the 1980s, she also wrote the wonderful <a href="https://www.sherryturkle.com/alone-together"><i>Alone Together</i></a> at the drawn on the social media period. I take her perspectives on tech quite seriously, and you should too...)
				<br>
				<br>
				The answer to both of these 'why not just...' questions is that it is cheaper to use AI to continue providing crappy services than it is to invest in making these various services better. Such an economic bind is at the heart of why generative AI is not a radically transformative technology--it is being used to paper over the cracks of shitty jobs, shitty bosses, shitty companies and shitty public services. The absence of an economic and political will to use AI in a genuinely transformative way is not there, and its absence is evidenced by the shittiness of the things to which AI is now being applied.
				<br>
				<br>
				The funny thing about all this is it's essentially a repeat of the 'revolution' which happened with computers several decades ago. To mentioned another paradox, the <a href="https://en.wikipedia.org/wiki/Productivity_paradox">Solow paradox</a> was coined when economist Robert Solow wrote that 'we can see the computer age everywhere except in the productivity statistics.' Thus, the paradox asks: why, despite so much investment in computing technology, have we not seen sizeable increases in productivity? A few answers to this question have emerged. For instance, it <a href="https://press.princeton.edu/books/hardcover/9780691172798/the-technology-trap">Carl Benedikt Frey discusses</a> how it takes time for the skils to use new technologies to become common in the labour market, and thus cheap enough for technologies to be productively utilised. I like this argument a lot. However, economists like <a href="https://www.aeaweb.org/articles?id=10.1257/aer.104.5.394">Daron Acemoglu have argued</a>, quite recently, that the Solow paradox is <i>still</i> plaguing computers nearly 30 years after Solow coined the term. For Acemoglu et al., the problem is, well, <i>complicated</i>, but part of it is that a small number of workers become extremely productive when given a computer, while most see no real benefit, meaning the overall productivity gain is very small (Acemoglu has also argued, rightly in my opinion, that the tendency to use technology to automate labour essentially <a href="https://www.nber.org/papers/w16082">locks in productivity at a particular level</a>, whereas augmenting labour with technology may allow workers to find <i>even more productive</i> uses of technology, achieving a higher overall degree of productivity increase. Essentially, managerial short-term gain is trumping greater gains in the long-term. See the paragraph immediately above this one. Related to this, computers are increasingly used to create work that <i>cannot</i> be made that much more efficient. Deliveroo could not exist without computing technology, but a Deliveroo driver can't really be made more efficient, no matter how many microchips and flashing lights you throw at them. Thus, in terms of the jobs that computing creates, it is few very high productivity coding jobs, and many low productivity manual labour jobs--again, a polarisation of productivity a la Acemoglu's work).
				<br>
				<br>
				<b>A Brief Note on What Would Be Required for Radically Transformative AI</b>
				<br>
				<br>
				In the mid-twentieth century, the cyberneticist Stafford Beer saw something like this coming. As <a href="https://mitpress.mit.edu/9780262525961/cybernetic-revolutionaries/">Medina (2014, p. 29)</a>, writing on the history of Beer's cybernetics, notes: "Beer argued that most applications [of computers] simply automated existing procedures and operations within [firms] instead of taking advantage of the new capabilities offered by computer technology to envision new forms of organization and better methods of management. Applied differently, computer technology could help organise the parts of the business into a better-functioning whole and allow companies to focus on the future instead of compiling pages of data that documented past performance. Computers did not need to reinforce existing management hierarchies and procedures; instead, they could bring about structural transformation within a company... [Beer's] focus was not on creating more advanced machines but rather on existing computer technologies to develop more advanced systems of organization." Again, as I think I have <a href="https://siu2lh.com/articles/empowering-everyday-experts.html">previously written</a>, similar things were discussed by Herbert Simon (one of the father's of AI) on using <a href="https://www.jstor.org/stable/25060980">human-AI 'collaboration'</a> in organisations. These perspectives do not merely <i>label</i> AI and computing as radical, transformative technologies; they put their money were their mouth is, and suggest that these technologies should enter into organisations as a force for total <i>reorganisation</i>. As technologies that do not merely change <i>how</i> something is done, but <i>what</i> is done to begin with.
				<br>
				<br>
				Inevitability, such ambition requires a confrontation with political economic forces. Managers have to be willing to give up power, to delegate responsiblity, while both managers and workers have to be willing to do different things. Comfortable executives have to be willing to take risks, and they have to be orientated towards a longer-term, augmentation view of technology, which present market incentives simply to do encourage. Some of these factors contribute to why, <a href="https://direct.mit.edu/books/monograph/2535/From-Newspeak-to-CyberspeakA-History-of-Soviet">according to historian Slava Gerovitch</a>, attempts to introduce widespread computing into the Soviet economy failed--it would entail entrenched interests giving up some of their power and doing things differently. One might contend that the absence of the Soviet Union has allowed liberal capitalism to become sclerotic, hence the current managerial tendencies which govern many technology applications today. I think this is a bridge too far, though it is without question that political economy plays a role. <a href="https://basicbooks.uk/titles/daron-acemoglu/power-and-progress/9781399804493/">Acemoglu and Johnson (2023)</a>, for instance, argue that the doctrine of neoliberal economics which has become endemic in US business schools since the 1980s explains why so many executives today prioritise using technology to automate work (which, again, produces short-term cost-savings at the expense of long-term productivity gains) over augmenting work.
				<br>
				<br>
				Generative AI is not going to change any of this, and in the absence of change, in the absence of political will, economic incentives, and general imagination over the possibility of technology, generative AI will never be a genuinely, radically transformative technology.
			</p>
		</div>

		<h1>
			<a href="#" onclick="history.back()"; return false;><font color="FFFFFF">back</font></a>
			<font color="FF7A7A">.</font>
		</h1>

	</center>
	<br>
	<br>
</body>
</html>