<!DOCTYPE html>
<html>
<head>
	<meta name="viewpoint" content="width=device-width, initial-scale=1.0">
	<style>
		body {
			background-color: #000000;
		}
		h1 {color: #000000;}
		p {
			color: #FFFFFF;
			font-family: "Georgia";
		}

		a:link {
			color: #FF7A7A;
			background-color: transparent;
			text-decoration: none;
		}
		
		a:visited {
			color: #FF7A7A;
			background-color: transparent;
			text-decoration: none;
		}

		a:hover {
			color: #00FFFF;
			background-color: transparent;
			text-decoration: none;
		}

		.h1_recentangle {
			height: 80px;
			width: 100%;
			background-color: #000000;
			text-align: center;
			justify-content: center;
			align-items: center;
			display: flex;
			margin: 0;
		}

		.post_window {
			height: 80%;
			width: 50%;
			background-color: #000000;
			border: solid 2px #FFFFFF;
			padding: 50px;
			text-align: justify;
			text-justify: inter-word;
			align-items: center;
			display: flex;
			transition: width 0.5s, height 0.5s;
			box-shadow: 0 0 2px #FFFFFF;
		}

		.post-title {
			font-weight: bold;
			font-size: 24px;
			font-family: "Georgia";
		}

	</style>
	<title>Some Problems Aren't Behavioural</title>
	<link rel="icon" type="image/x-icon" href="images/favicon.png">
</head>

<body>
	<div class="h1_recentangle" onclick="history.back()"; return false;>
		<h1>
			<a href="#" onclick="history.back()"; return false;><font color="FF7A7A">.</font></a>
		</h1>
	</div>
	<br>
	<center>

		<div class="post_window">
			<p>
				<span class="post-title">Some Problems Aren't Behavioural</span>
				<br>
				<br>
				<b>Introduction</b>
				<br>
				I am critical</a> of efforts to solve behavioural problems with technical solutions. Indeed, many instances of technological adoption in organisations today respond to organisational (i.e., human) challenges, which for one reason or another cannot be solved through human means (typically, because the person causing the problem is also the person with the power to choose the solution, and we don't like pointing the finger at ourselves).
				<br>
				<br>
				This post isn't about poor technical solutions. Rather, it's about poor behavioural solutions/diagnoses to technical problems. Because, just as behavioural problems are ill-served by technical fixes, so too are technical problems ill-served by behavioural strategies. By 'technical problems' and 'technical solutions,' I <i>may</i> mean things like changing website functionality. But more often, I mean things like legal reform, regulatory change, economic reorganisation. In short, stuff that changes our <i>relationship</i> with technology, rather than just our <i>experience</i> of it (which, in my opinion, is often what behavioural interventions typically do).
				<br>
				<br>
				There is a great deal of enthusiasm amongst behavioural scientists to use the field to solve important problems in the world. But their toolbox is, understandably, limited to behavioural tools. Of course, this problem is faced by all professions. Financiers will probably diagnose incentive problems. Leadership experts will point blame at whoever is in charge. Social critics will emphasise malign institutions and cultural trends. We all approach scenarios with our biased perspectives, which I mean in the <a href="https://search.worldcat.org/title/356505">Simon (2000)</a> sense of <i>our default view of the facts and values at play</i>. So, this is not an attack on behavioural science <i>per se</i>. It just happens to be a field I am more familiar with, and thus more exposed to.
				<br>
				<br>
				<b>Online Influencers and Dark Patterns</b>
				<br>
				Behavioural science has applications in online retail, for good and for ill. One example which I generally take a neutral stance on is influencer marketing. I have supervised various student projects examining the topic. Invariably, these discussions focus on <i>why</i> this form of marketing works. There are discussions of warm-glow effects (e.g., <a href="https://doi.org/10.2307/2234133">Andreoni, 1990</a>), parasociality (e.g., <a href="https://doi.org/10.1016%2Fj.intmar.2013.12.003">Labrecque, 2014</a>), and so on. These areas of the topic are typically the ones which <i>actually</i> interest the student.
				<br>
				<br>
				But, do these behavioural aspects matter? This is a question which I have found myself, first gently and now more explicitly, bringing up over the years. Much of the 'behavioural science' of influencer marketing isn't especially new. Many of the papers which find themselves cited in these discussions are several decades old, while the discussions themselves are typically (good) summaries of a huge amount of literature on, essentially, <i>why people like one another</i>. <a href="https://en.wikipedia.org/wiki/Parasocial_interaction">Wikipedia informs me</a> that the term 'parasocial' <a href="https://doi.org/10.1080%2F00332747.1956.11023049">was coined in 1956</a>. That <a href="https://en.wikipedia.org/wiki/Warm-glow_giving">same knowledge aggregator</a> begins the discussion of warm-glow effects with Socrates, with economics grounded in Ricardo. Personally, I recall working on a project about metacognition several years ago, and all the major literature came from the 1990s. It essentially said 'trust matters, and trust is influenced by expertise' (<a href="https://psycnet.apa.org/doi/10.1086/209380">Friestad and Wright, 1994</a>, <a href="https://doi.org/10.1002/(SICI)1520-6793(199903)16:2%3C185::AID-MAR7%3E3.0.CO;2-N">1999</a>; <a href="https://search.worldcat.org/title/748772904">Moon, 2010</a>).
				<br>
				<br>
				This is all to say, before Facebook or Instagram or TikTok, most marketing professors could come up with a reasonable theory for why 'influencer marketing' would work, and it would essentially be identical to the theories which are used today.
				<br>
				<br>
				Of course, 'influencer marketing' wasn't really around in the 1990s. If it is a phenomenon today, <i>something</i> must explain what is happening. And that something, in my mind, is almost entirely technical. One of the reasons platforms--which economists used to call two-sided markets--work is because they lower transaction costs for disparate groups (<a href="https://web.mit.edu/14.271/www/rochet_tirole.pdf">Rochet and Tirole, 2004</a>). If I want to buy a product, and you want to sell a product, we could potentially help one another out, but we'd need to find one another first. Platfoms like Facebook marketplace, Amazon, or eBay, all offer a coordinating advantage in this regard. This is very relevant for influencer marketing. Without such platforms, the coordination costs of getting an influencer and a potential consumer in the same place with be inhibative. Perhaps the most famous analogue would be something like the 'Avon Ladies' who would patrol their territory, arranging Avon parties at which they would huck their wares to a group of potential customers. Avon parties also demonstrate another technological advantage for platform-based nfluencer marketing: <i>scale</i>. It is more expensive to pay someone to do a personable sales pitch than it is to put up a billboard. Thus, to make influencer marketing economically viable, the number of attendees to the pitch has to be huge. Imagine an Avon party, hosted by one person, for tens of thousands of attendees. Physically, such a proposition is unreasonable. But via a digital platform, this becomes technically viable, and thus economically viable.
				<br>
				<br>
				In my opinion, these two reasons explain everything. Before platforms, coordination costs were too high and the scale was too small for influencer marketing to be viable. Platforms lowered costs, increased scale, and thus turned influencer marketing into a more viable sales strategy. No new psychology has really driven this trend. Insofar as influencer marketing is a 'problem' (which I'm not saying it is), to tackle it behaviourally seems strange. People have always been influencing one another, buying based on recommendations, demonstrations, and commentaries (<a href="https://search.worldcat.org/title/748772904">Moon, 2010</a>). If one wanted to end influencer marketing tomorrow, one should not 'nudge' users of a platform. Rather, one should change the technical aspects of the platform, say, by setting a low maximum number of followers, or capping the number of viewers in a livestream.
				<br>
				<br>
				As above, I don't really think influencer marketing is a problem, though I do have concerns about aspirations to become an influencer. It is a job which only exists when relatively few people do it (<a href="https://www.penguinrandomhouse.com/books/97779/you-are-not-a-gadget-by-jaron-lanier/">Lanier, 2009</a>). It is a cannibalising profession. Yet, for something more on the dark side, we should look at dark patterns. In recent years, my work has focused more on thinking about these deceptive user interface designs. Indeed, <a href="https://doi.org/10.1017/bpp.2023.24">my work with colleagues</a> has played a small role in highlighting the alignment between the dark patterns literature and behavioural science. But this is a double-edged sword. Like with influencer marketing, I increasingly find myself roped into discussions about how behavioural science can tackle dark patterns. And, maybe behavioural science can--I know too little to comment authoritatively on some proposals for <a href="https://fairpatterns.com/">'fair patterns'</a> and 'light patterns,' (<a href="https://doi.org/10.1093/jla/laaa006">Luguri and Strahilevitz, 2021</a>) but I would not forestall the possibility that these ideas have potential.
				<br>
				<br>
				Yet, behavioural factors are not placed as the primary driver by whose job it is to care about dark patterns, namely, consumer regulators. I think for good reason. The UK's <a href="https://competitionandmarkets.blog.gov.uk/2022/04/07/online-choice-architecture-how-do-we-end-up-making-decisions-we-dont-want/">Competition and Market's Authority</a>, for instance, has recognised that the kind of 'tricks' which are characteristic of a dark pattern have existed pretty much for as long as markets have been around. Obscuring prices and upselling are not recent inventions. So, why do dark patterns matter, now? Well, for the same reason that influencer marketing now matters--technologies enable scales which have previously been unrealisable. One dodgy merchant with a market stall is regretable; when that merchant can sell to millions of people, we have a risk of significant consumer harm.
				<br>
				<br>
				To this end, I must ask--<i>are the solutions to dark patterns likely to be behavioural?</i> In the round, I suspect not. I suspect if the primary lever to tackle dark patterns were various nudges and other behavioural techniques, the net result would be a horrific mess of frameworks and taxonomies, legislative cases and a proliferation of definitions. I am, of course, biased here: <a href="https://doi.org/10.1111/rego.12590">my own advocacy</a> for regulatory principles as a solution is publicly available. In part, I'm not especially interested in discussing 'how to solve dark patterns' here. In fact, the perspective I am offering probably suggests we <i>can't</i> eliminate dark patterns. What I want to emphasise is that despite behavioural science having a function in the dark patterns discussion, the problem is a technical one. And thus a technical solution, such as regulatory intervention, is likely to be more appropriate. At the least, when behavioural scientists discuss dark patterns, they should be mindful of giving their discipline the appropriate role--<i>as a mechanism, not as a driver</i>.
				<br>
				<br>
				<b>Misinformation and Political Polarisation</b>
				<br>
				From the outset I should admit that I do not much care for much of the behavioural science work around misinformation and political polarisation. As a brief critique, far too much of it strikes me as a bit condescending. Misinformation scholars, whether explicitly stated in their work, are (in my experience) privately motivated by a belief which essentially boils down to 'people are easily manipulated, and that's why they don't agree with me.' I have no doubts that this does not describe <i>every</i> behavioural science of misinformation scholar, and as above, this perspective seems to be more of a private view than a scholarly one, but nevertheless, it rubs me the wrong way. Though--again, from personal experience--those who write about the drivers of political polarisation, from a behavioural perspective, seem more overt in expressing this sentiment. Many political scientists examine why society is polarised around various issues. I have no doubts that <i>some</i> aspect pertains to bias, or whatever. But (and again, maybe I hang out with the wrong people), I find enthusiasm for integrating history, economics, and sociology into explanations of political polarisation lacking amongst some behavioural individuals.
				<br>
				<br>
				Yet, I think this personal gripe extends to quite a legitimate critique. I was speaking to a friend several months ago who has been doing various work on political polarisation. They suggested that the apparent increase in liberal values in academia (I am taking their word for this) suggests that academics are becoming more closed-minded. By excluding right-wing perspectives, we as an academy are missing out on something. And while I don't disagree in principle, their perspective that this political shift reflects a behavioural anomaly (e.g., closed-mindedness) struck me as quite odd. Wages are falling, prices--particularly of food and housing--are increasing, while job security in UK HE is poor and the costs of getting to a high level in UK HE are greater than ever, due to higher fees and more competition. With my Marxist hat on, I can't help but wonder whether pathologising swathes of doctors and professors with 'closed-mindedness,' rather than chalking shifting political views up to tangible, material factors, <i>is not a bit of a stretch</i> (for the record, just because someone has a PhD, it does not magically mean they're <i>actually open-minded</i>).
				<br>
				<br>
				Of course, for the behavioural scientist, linking political polarisation to something describable as a behavioural bias is great. Maybe, if my disagreements with billionaires are only the result of poor mental shortcuts rather than, I don't know, substantially different class interests and economic power, we can all be singing kumbaya in no time. Now, to be fair, this is an exaggeration of the position held by my friend (who is a respected behavioural scientist, and whose writing is often both extremely accessible and internally coherent). Their concern is that we extrapolate far too much from too little information--for instance, that right-wingers are stupid--which I will concede has some worthwhile behavioural underpinnings if we go back again to, say, <a href="https://mitpress.mit.edu/9780262691918/the-sciences-of-the-artificial/">Simon (1981)</a> (I have heard such extrapolation by called the Simon paradox, but I'm not sure this is a commonly used term). I have no objection with this. But, as above, humans <i>have always been like this</i>. To highlight <i>this</i> as a driving feature of polarisation <i>now</i> is to try and explain variance with a constant. We have to go deeper, and that includes looking at the structural critique.
				<br>
				<br>
				Now, continuing my desire for fairness, my colleague did also suggest technology played a role. They have observed, for instance, that social interactions increasingly happen online, and that technology has a scaling component which perhaps causes us to lean more on some of these behavioural extrapolation mechanisms. This is a nice perspective, one that ties the technical and the behavioural together. Though, I would and have suggested they must take this much further. Namely, that if we ignore all the economic/sociological factors involved in polarisation, <i>this</i> technological explanation is <i>much</i> more relevant than a behavioural one. As technology critic <a href="https://www.penguinrandomhouse.com/books/97779/you-are-not-a-gadget-by-jaron-lanier/">Jaron Lanier (2009)</a> has emphasised, web2 has morphed human identity into a series of categories. Even our interactions with one another (e.g., like, share) are typically categoric. Humans can like things to various degrees, but our online personas can <i>only</i> like things, or not (also see <a href="https://www.penguin.co.uk/books/420675/the-four-dimensional-human-by-laurence-scott/9780099591894">Scott, 2015</a>). This is not an environment which encourages one to think about two as if two were a whole, rounded individual, rather than a series of categories. 
				<br>
				<br>
				Technology is the variable, behaviour is the constant. If anything, the behavioural tricks which cause us to elaborate from very little information are a survival mechanism for a deeply <i>inhuman</i> online environment. The solution to polarisation, if again it is not a product of structural economic factors, must be a <i>technical</i> solution, such as redesigning online platforms, online interactions, or even the pervasiveness of the online in the offline world. A society that rejected certain uses of the online space--news, political commentary, professional working, commercial advertising--would regard the internet quite differently, and would define challenges like polarisation and misinformation very differently. If we have a grasp of how humans behave, the policy question should be <i>how should we allow technology to be designed and used?</i> It should not be <i>how can we alter/manage human behaviour?</i> (see, for instance, <a href="https://www.cambridge.org/core/books/reengineering-humanity/379F3C68F6AAC6C0C3998C14DACC38CF">Frischmann and Selinger, 2018</a>)
				<br>
				<br>
				The story with misinformation is, I think, quite similar. Misinformation has <i>always</i> existed, and insofar as there are novel factors which are potentially driving misinformation, these are generally technical factors. For instance, in a recent talk I gave at a Fringe Turing Institute event, the topic of personalised misinformation came up, something which the abovementioned regulatory bodies are probably concerned with too. In this instance, technology <i>exacerbates</i> already existing behavioural phenomena, and points to a <i>technical</i> solution, not necessarily a behavioural one. 
				<br>
				<br>
				An interesting paper on this topic comes from <a href="https://www.magdaosman.com/_files/ugd/8b1381_13be0d2bb5934d378b75164a8c95eaa2.pdf">Adams <i>et al</i>., 2023</a>. In their review of various misinformation studies, they reaffirm that really, information technology is at the heart of the matter. But also, crucially, they note that there isn't really a strong evidential base around the behavioural science of it all. We don't really know how 'believing' misinformation (insofar as one retweets or shares a piece of fake news) effects what that person and others really do in their everyday lives. This reminds me of the findings of the UK's Information Commissioner's Office which found <a href="https://www.ft.com/content/aa235c45-76fb-46fd-83da-0bdf0946de2d">no evidence</a> linking the psychographics firm Cambridge Analytica to voting preferences in the Brexit referendum. A <a href="https://www.newyorker.com/magazine/2024/04/22/dont-believe-what-theyre-telling-you-about-misinformation">recent piece</a> in <i>The New Yorker</i> is also worth reading, on this matter.
				<br>
				<br>
				<a href="https://www.magdaosman.com/_files/ugd/8b1381_13be0d2bb5934d378b75164a8c95eaa2.pdf">Adams <i>et al</i>. (2023)</a> raise an additional, interesting point which I generally think is worth consideration. Namely, that even if technology has a role in this whole story, emphasising technology as a magnifying glass for behavioural biases ignores the role of history, sociology, philosophy of science, epistemology, and as above, economics. Indeed, it is worth asking ourselves 'who decides what is true?' Only those who have already convinced themselves that they are privy to 'reality' can position themselves as the saviours of those otherwise damned to suffer fake news in their social media feeds (<a href="https://www.versobooks.com/en-gb/products/1069-science-in-a-free-society">Feyerabend, 1978</a>).
				<br>
				<br>
				For someone like <a href="https://www.versobooks.com/en-gb/products/1069-science-in-a-free-society">Feyerabend (1978)</a>, if not for someone like <a href="https://monoskop.org/images/1/17/Illich_Ivan_Deschooling_Society.pdf">Illich (1971)</a>, the solution to a 'problem' like misinformation or political polarisation is not for one group to diagnose and treat the other through a prescription of debiasing interventions promoted via TEDTalks and book tours. Rather, it is to actually <i>accept</i> that people can disagree, and to be willing to consider that those disagreements arise for legitimate reasons. This is a much bigger and more humbling ask than just nudging someone to reconsider before they share an article about Joe Biden being a lizard, or whatever.
				<br>
				<br>
				<b> Some Conclusions</b>
				<br>
				What I am trying to articulate with these various examples is that some--perhaps not all, but certainly <i>some</i>--of the topics that dominate behavioural science today probably aren't all that behavioural. There is a behavioural component at play, though as behavioural scientists love to remind people, <i>behaviour is always important</i>. What matters more is what factors cause the behaviour to be expressed, or exacerbated.
				<br>
				<br>
				Society has never been perfect, and I do not want to imply an idyllic time prior to the emergence of some technologies. But it seems reasonable to me that if there was a time when humans could be biased, influenced by others, extrapolate from little information, or whatever; if there was a time when these behaviours did <i>not</i> result in the end of the world, then these behavioural 'problems' today are not really behavioural. They are technical, in the sense that they require some adjustment in how technologies, institutions, and laws interact to shape human experiences. As above, it is natural for a well-meaning behavioural science to prescribe a behavioural solution to whatever problem they care about, just as it is reasonable for an anthropologist to derive a solution from anthropology. But what matters--what always matters--is getting the right 'problem representation' to use the language of <a href="https://mitpress.mit.edu/9780262691918/the-sciences-of-the-artificial/">Simon (1981)</a>. One needs to understand the problem before a solution can be determined.
			</p>
		</div>

		<h1>
			<a href="#" onclick="history.back()"; return false;><font color="FFFFFF">back</font></a>
			<font color="FF7A7A">.</font>
		</h1>

	</center>
	<br>
	<br>
</body>
</html>